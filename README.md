# RAG v3 - Production-Grade Vietnam Labor Law QA System

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-blue.svg" alt="Python">
  <img src="https://img.shields.io/badge/LlamaIndex-0.10.x-green.svg" alt="LlamaIndex">
  <img src="https://img.shields.io/badge/FastAPI-0.109+-red.svg" alt="FastAPI">
  <img src="https://img.shields.io/badge/Chainlit-1.0+-purple.svg" alt="Chainlit">
</p>

## ğŸ›ï¸ Overview

RAG v3 is a **production-grade** Retrieval-Augmented Generation system for querying Vietnam Labor Law 2019. It features an **agentic architecture** with semantic routing, conversational memory, and hybrid search capabilities.

### Key Features

| Feature | Description |
|---------|-------------|
| ğŸ¯ **Semantic Router** | LLM-based intent classification (CHAT vs LAW) |
| ğŸ’¬ **Conversational Memory** | CondensePlusContextChatEngine with query rewriting |
| ğŸ” **Hybrid Search** | Vector (Qdrant) + BM25 + Reciprocal Rank Fusion |
| ğŸ¯ **BGE Reranker** | BAAI/bge-reranker-v2-m3 for result refinement |
| ğŸ“Š **Observability** | Arize Phoenix tracing integration |
| âš¡ **Streaming** | Full async streaming from backend to frontend |

## ğŸ“ Project Structure

```
RAG_v3/
â”œâ”€â”€ .env.example                # Environment config template
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # This file
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ VIETNAM_LABOR_LAW.pdf   # Source document
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ ingest.py               # Offline PDF â†’ Qdrant ingestion
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py               # Pydantic Settings
â”‚   â”œâ”€â”€ main.py                 # FastAPI entrypoint + Phoenix setup
â”‚   â”‚
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ routes.py           # API endpoints (/chat, /query, /health)
â”‚   â”‚   â””â”€â”€ schemas.py          # Pydantic request/response models
â”‚   â”‚
â”‚   â””â”€â”€ engine/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ components.py       # LLM, Embedding, Reranker factories
â”‚       â”œâ”€â”€ retriever.py        # HybridRetriever (Vector + BM25 + RRF)
â”‚       â””â”€â”€ chat_engine.py      # SemanticRouter + CondensePlusContextChatEngine
â”‚
â””â”€â”€ frontend/
    â”œâ”€â”€ app.py                  # Chainlit UI application
    â”œâ”€â”€ .env.example            # Frontend config
    â””â”€â”€ .chainlit/
        â””â”€â”€ config.toml         # Chainlit UI configuration
```

## ğŸš€ Quick Start

### 1. Prerequisites

- Python 3.10+
- Qdrant Cloud account (or local Qdrant)
- Google Gemini API key (or OpenAI)

### 2. Installation

```bash
# Clone or navigate to RAG_v3
cd RAG_v3

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or: venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

### 3. Configuration

```bash
# Copy environment template
cp .env.example .env

# Edit .env with your API keys
nano .env  # or use your preferred editor
```

Required environment variables:
```env
GEMINI_API_KEY=your-gemini-api-key
QDRANT_URL=https://your-cluster.qdrant.io
QDRANT_API_KEY=your-qdrant-api-key
```

### 4. Ingest Data

Place your PDF in `data/VIETNAM_LABOR_LAW.pdf`, then run:

```bash
python scripts/ingest.py --pdf data/VIETNAM_LABOR_LAW.pdf
```

### 5. Start Backend Server

```bash
# From project root
python -m src.main

# Or with uvicorn directly
uvicorn src.main:app --host 0.0.0.0 --port 8000 --reload
```

The API will be available at:
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

### 6. Start Frontend

```bash
# In a new terminal
cd frontend
cp .env.example .env
chainlit run app.py --port 8501
```

Visit http://localhost:8501 to start chatting!

## ğŸ”Œ API Endpoints

### POST /chat
Main chat endpoint with semantic routing.

```json
{
  "message": "Thá»i gian lÃ m viá»‡c tá»‘i Ä‘a trong má»™t tuáº§n?",
  "session_id": "optional-session-id",
  "stream": false,
  "skip_routing": false
}
```

Response:
```json
{
  "answer": "Theo Äiá»u 105, Khoáº£n 1...",
  "intent": "LAW",
  "source_nodes": [...],
  "session_id": "..."
}
```

### POST /query
Simple query (backward compatible with v2).

```json
{
  "question": "Quyá»n cá»§a ngÆ°á»i lao Ä‘á»™ng?",
  "top_k": 5
}
```

### POST /reset-memory
Reset conversation history.

### GET /health
Health check endpoint.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             â”‚     â”‚             â”‚     â”‚           RAG Engine            â”‚
â”‚   Chainlit  â”‚â”€â”€â”€â”€â–¶â”‚   FastAPI   â”‚â”€â”€â”€â”€â–¶â”‚                                â”‚
â”‚   Frontend  â”‚     â”‚   Backend   â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚             â”‚â—€â”€â”€â”€â”€â”‚             â”‚â—€â”€â”€â”€â”€â”‚  â”‚ Semantic â”‚  â”‚CondensePlusâ”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”‚  Router  â”‚â”€â–¶â”‚ ChatEngine â”‚  â”‚
                                        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                        â”‚        â”‚              â”‚        â”‚
                                        â”‚        â–¼              â–¼        â”‚
                                        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                                        â”‚  â”‚    Hybrid Retriever      â”‚  â”‚
                                        â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
                                        â”‚  â”‚  â”‚ Vector â”‚ â”‚  BM25  â”‚   â”‚  â”‚
                                        â”‚  â”‚  â”‚ Search â”‚ â”‚ Search â”‚   â”‚  â”‚
                                        â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
                                        â”‚  â”‚         â”‚ RRF â”‚          â”‚  â”‚
                                        â”‚  â”‚         â–¼     â–¼          â”‚  â”‚
                                        â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚  â”‚
                                        â”‚  â”‚    â”‚  Reranker  â”‚        â”‚  â”‚
                                        â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚  â”‚
                                        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                        â”‚                             â”‚
                                        â–¼                             â–¼
                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                  â”‚  Qdrant  â”‚                 â”‚  Gemini  â”‚
                                  â”‚  Cloud   â”‚                 â”‚   LLM    â”‚
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Observability (Phoenix)

Start Arize Phoenix for tracing:

```bash
# Install Phoenix
pip install arize-phoenix

# Start Phoenix server
phoenix serve

# Phoenix UI: http://localhost:6006
```

Configure in `.env`:
```env
PHOENIX_COLLECTOR_ENDPOINT=http://localhost:6006/v1/traces
ENABLE_TRACING=true
```

## ğŸ”§ Configuration Reference

| Variable | Default | Description |
|----------|---------|-------------|
| `LLM_PROVIDER` | `gemini` | LLM provider (gemini/openai) |
| `LLM_MODEL_GEMINI` | `models/gemini-1.5-flash` | Gemini model |
| `LLM_MODEL_OPENAI` | `gpt-4o-mini` | OpenAI model |
| `EMBEDDING_MODEL` | `bkai-foundation-models/vietnamese-bi-encoder` | Vietnamese embedding |
| `RERANKER_MODEL` | `BAAI/bge-reranker-v2-m3` | Reranker model |
| `VECTOR_TOP_K` | `20` | Vector search results |
| `BM25_TOP_K` | `20` | BM25 search results |
| `RERANKER_TOP_N` | `5` | Final reranked results |
| `RRF_K` | `60` | RRF fusion constant |

## ğŸ“ License

MIT License - See LICENSE file for details.

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request
