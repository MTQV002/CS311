#!/usr/bin/env python3
"""
RAG v3 - Ingestion Pipeline (DOCX Version)
==========================================
ChuyÃªn dá»¥ng cho file Word (.docx) Bá»™ luáº­t Lao Ä‘á»™ng.
Æ¯u Ä‘iá»ƒm:
- Äá»c text chÃ­nh xÃ¡c 100% (khÃ´ng bá»‹ lá»—i OCR/Scan).
- Tá»± Ä‘á»™ng chuáº©n hÃ³a Unicode tiáº¿ng Viá»‡t.
- Táº¡o ID chuáº©n UUID cho Qdrant.
"""
import sys
import os
import argparse
import re
import uuid
import unicodedata
from pathlib import Path
from typing import List, Dict, Optional
from tqdm import tqdm

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# ThÆ° viá»‡n Ä‘á»c file Word
try:
    import docx
except ImportError:
    print("âŒ Lá»—i: ChÆ°a cÃ i thÆ° viá»‡n python-docx.")
    print("ğŸ‘‰ Vui lÃ²ng cháº¡y: pip install python-docx")
    sys.exit(1)

from llama_index.core.schema import TextNode
from llama_index.core import VectorStoreIndex, StorageContext
from llama_index.vector_stores.qdrant import QdrantVectorStore
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams

from src.config import settings


class VietnamLaborLawDocxParser:
    def __init__(self, file_path: str):
        self.file_path = file_path
        print(f"ğŸ“„ Loading DOCX: {file_path}")
        try:
            self.doc = docx.Document(file_path)
        except Exception as e:
            print(f"âŒ KhÃ´ng thá»ƒ má»Ÿ file Word: {e}")
            sys.exit(1)
        self.full_text = ""
    
    def extract_full_text(self) -> str:
        """Äá»c toÃ n bá»™ text tá»« cÃ¡c Ä‘oáº¡n vÄƒn (paragraph) trong file Word"""
        print("â³ Extracting text from paragraphs...")
        
        text_parts = []
        for para in tqdm(self.doc.paragraphs, desc="Reading paragraphs"):
            # Chá»‰ láº¥y cÃ¡c dÃ²ng cÃ³ ná»™i dung (bá» dÃ²ng trá»‘ng)
            clean_text = para.text.strip()
            if clean_text:
                text_parts.append(clean_text)
            
        raw_text = "\n".join(text_parts)
            
        # 1. Chuáº©n hÃ³a Unicode (Quan trá»ng cho tiáº¿ng Viá»‡t: Tá»• há»£p -> Dá»±ng sáºµn)
        self.full_text = unicodedata.normalize('NFKC', raw_text)
        
        # 2. Xá»­ lÃ½ khoáº£ng tráº¯ng Ä‘áº·c biá»‡t (Non-breaking space)
        self.full_text = self.full_text.replace('\xa0', ' ')
        
        # 3. Xá»­ lÃ½ xuá»‘ng dÃ²ng thá»«a
        self.full_text = re.sub(r'\n{3,}', '\n\n', self.full_text)
        
        return self.full_text
    
    def parse_hierarchical(self) -> List[Dict]:
        """PhÃ¢n tÃ­ch cáº¥u trÃºc Äiá»u/Khoáº£n tá»« vÄƒn báº£n Ä‘Ã£ lÃ m sáº¡ch"""
        text = self.full_text
        chunks = []
        
        # Regex Patterns (ÄÃ£ tá»‘i Æ°u cho tiáº¿ng Viá»‡t)
        chapter_pattern = r'ChÆ°Æ¡ng\s+([IVX0-9]+)(?:[:.\s]+([^\n]*))?'
        section_pattern = r'Má»¥c\s+(\d+)(?:[:.\s]+([^\n]*))?'
        article_pattern = r'(?:Äiá»u|ÄIá»€U)\s+(\d+)\s*[.:]?\s*(.*?)(?=(?:(?:Äiá»u|ÄIá»€U)\s+\d+|ChÆ°Æ¡ng\s+[IVX0-9]+|$))'
        
        flags = re.IGNORECASE | re.MULTILINE | re.DOTALL

        # QuÃ©t cáº¥u trÃºc tá»•ng thá»ƒ
        chapters = [(m.start(), m.group(1), m.group(2).strip() if m.group(2) else "") 
                    for m in re.finditer(chapter_pattern, text, flags)]
        
        # QuÃ©t cÃ¡c Äiá»u luáº­t
        articles = list(re.finditer(article_pattern, text, flags))
        
        print(f"ğŸ“Š Found {len(chapters)} chapters, {len(articles)} articles")
        
        # FALLBACK: Náº¿u regex tháº¥t báº¡i (dÃ¹ file docx Ã­t khi bá»‹), dÃ¹ng Sliding Window
        if len(articles) < 5:
            print("âš ï¸  Cáº£nh bÃ¡o: KhÃ´ng tÃ¬m tháº¥y Ä‘á»§ cáº¥u trÃºc Äiá»u luáº­t. Chuyá»ƒn sang cháº¿ Ä‘á»™ Cáº¯t LÃ¡t (Sliding Window).")
            return self._sliding_window_chunking(text)

        # PhÃ¢n tÃ­ch chi tiáº¿t tá»«ng Äiá»u
        for article_match in tqdm(articles, desc="Parsing articles"):
            article_num = article_match.group(1)
            article_content = article_match.group(2).strip()
            article_pos = article_match.start()
            
            # TÃ¬m chÆ°Æ¡ng chá»©a Ä‘iá»u nÃ y
            current_chapter = next((c for c in reversed(chapters) if c[0] < article_pos), (None, "?", ""))
            
            # TÃ¡ch Khoáº£n (1. abc...)
            clause_pattern = r'^(\d+)\.\s+(.+?)(?=(?:^\d+\.\s+|$))'
            clauses = list(re.finditer(clause_pattern, article_content, re.MULTILINE | re.DOTALL))
            
            meta = {
                "article": article_num,
                "chapter": current_chapter[1],
                "chapter_title": current_chapter[2],
                "source": "Vietnam Labor Law 2019 (DOCX)"
            }
            
            if clauses:
                for c_num, c_text in clauses:
                    if len(c_text.strip()) > 5:
                        full_content = (
                            f"ChÆ°Æ¡ng {current_chapter[1]}: {current_chapter[2]}\n"
                            f"Äiá»u {article_num}.\n"
                            f"Khoáº£n {c_num}. {c_text.strip()}"
                        )
                        chunk_meta = meta.copy()
                        chunk_meta.update({"clause": c_num, "type": "clause"})
                        chunks.append({"content": full_content, "metadata": chunk_meta})
            else:
                # Äiá»u khÃ´ng cÃ³ khoáº£n
                full_content = (
                    f"ChÆ°Æ¡ng {current_chapter[1]}: {current_chapter[2]}\n"
                    f"Äiá»u {article_num}.\n"
                    f"{article_content}"
                )
                chunk_meta = meta.copy()
                chunk_meta.update({"clause": None, "type": "article"})
                chunks.append({"content": full_content, "metadata": chunk_meta})
        
        return chunks

    def _sliding_window_chunking(self, text: str, chunk_size=1024, overlap=200):
        """Fallback an toÃ n: Cáº¯t vÄƒn báº£n thÃ nh cÃ¡c Ä‘oáº¡n chá»“ng láº¥p"""
        print(f"ğŸ”„ Running Sliding Window (Size={chunk_size})...")
        chunks = []
        start = 0
        text_len = len(text)
        
        while start < text_len:
            end = start + chunk_size
            chunk_text = text[start:end]
            
            # Cá»‘ gáº¯ng cáº¯t táº¡i dáº¥u xuá»‘ng dÃ²ng Ä‘á»ƒ cÃ¢u khÃ´ng bá»‹ gÃ£y
            last_newline = chunk_text.rfind('\n')
            if last_newline != -1 and last_newline > chunk_size * 0.5:
                end = start + last_newline + 1
                chunk_text = text[start:end]
            
            if len(chunk_text.strip()) > 50:
                chunks.append({
                    "content": chunk_text.strip(),
                    "metadata": {
                        "type": "sliding_window",
                        "source": "Vietnam Labor Law 2019 (Fallback)"
                    }
                })
            start = end - overlap
            
        return chunks


def create_nodes_from_chunks(chunks: List[Dict]) -> List[TextNode]:
    """Táº¡o Node LlamaIndex vá»›i ID lÃ  UUID chuáº©n"""
    nodes = []
    for chunk in chunks:
        node_id = str(uuid.uuid4()) # Táº¡o UUID ngáº«u nhiÃªn
        node = TextNode(
            text=chunk["content"],
            metadata=chunk["metadata"],
            id_=node_id,
            excluded_embed_metadata_keys=["source"],
            excluded_llm_metadata_keys=["source"]
        )
        nodes.append(node)
    return nodes


def get_qdrant_client() -> QdrantClient:
    if settings.QDRANT_API_KEY:
        print(f"â˜ï¸  Connecting to Qdrant Cloud: {settings.QDRANT_URL}")
        return QdrantClient(
            url=settings.QDRANT_URL,
            api_key=settings.QDRANT_API_KEY,
        )
    else:
        print(f"ğŸ–¥ï¸  Connecting to local Qdrant: {settings.QDRANT_URL}")
        return QdrantClient(url=settings.QDRANT_URL)


def get_embedding_model():
    from llama_index.embeddings.huggingface import HuggingFaceEmbedding
    print(f"ğŸ”¤ Loading embedding model: {settings.EMBEDDING_MODEL}")
    embed_model = HuggingFaceEmbedding(
        model_name=settings.EMBEDDING_MODEL,
        embed_batch_size=settings.EMBEDDING_BATCH_SIZE,
        trust_remote_code=True
    )
    print("âœ… Embedding model loaded")
    return embed_model


def ingest_to_qdrant(nodes, client, collection_name, embed_model):
    collections = [c.name for c in client.get_collections().collections]
    if collection_name in collections:
        print(f"âš ï¸  Collection '{collection_name}' exists. Deleting...")
        client.delete_collection(collection_name)
    
    print(f"ğŸ“¦ Creating collection: {collection_name}")
    client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=settings.EMBEDDING_DIM, distance=Distance.COSINE)
    )
    
    vector_store = QdrantVectorStore(client=client, collection_name=collection_name)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    
    print(f"ğŸ“¥ Ingesting {len(nodes)} nodes into Qdrant...")
    index = VectorStoreIndex(
        nodes=nodes,
        storage_context=storage_context,
        embed_model=embed_model,
        show_progress=True
    )
    return index


def main():
    # TÃªn file DOCX máº·c Ä‘á»‹nh
    DEFAULT_DOCX = "Bá»™-luáº­t-45-2019-QH14.docx"
    
    parser = argparse.ArgumentParser(description="Ingest Vietnam Labor Law DOCX into Qdrant")
    parser.add_argument(
        "--file",
        type=str,
        default=f"data/{DEFAULT_DOCX}",
        help="Path to the DOCX file"
    )
    args = parser.parse_args()
    
    file_path = Path(args.file)
    # Xá»­ lÃ½ Ä‘Æ°á»ng dáº«n tÆ°Æ¡ng Ä‘á»‘i tá»« project root
    if not file_path.is_absolute():
        file_path = PROJECT_ROOT / file_path

    if not file_path.exists():
        print(f"âŒ File not found: {file_path}")
        print(f"   Vui lÃ²ng copy file '{DEFAULT_DOCX}' vÃ o thÆ° má»¥c data/")
        sys.exit(1)
        
    collection_name = settings.QDRANT_COLLECTION
    
    print("=" * 60)
    print("ğŸš€ RAG v3 - Ingestion Pipeline (Word/DOCX Version)")
    print("=" * 60)
    
    # 1. Parse
    parser = VietnamLaborLawDocxParser(str(file_path))
    parser.extract_full_text()
    chunks = parser.parse_hierarchical()
    
    if not chunks:
        print("âŒ CRITICAL ERROR: No content extracted.")
        sys.exit(1)
        
    print(f"âœ… Generated {len(chunks)} chunks")
    
    # 2. Create Nodes
    print("\nğŸ”— Step 2: Creating LlamaIndex nodes...")
    nodes = create_nodes_from_chunks(chunks)
    
    # 3. Embed & Ingest
    print("\nğŸ§  Step 3: Loading embedding model...")
    embed_model = get_embedding_model()
    
    print("\nğŸ”Œ Step 4: Connecting to Qdrant...")
    client = get_qdrant_client()
    
    print("\nğŸ“¤ Step 5: Ingesting into Qdrant...")
    ingest_to_qdrant(nodes, client, collection_name, embed_model)
    
    print("\nğŸ‰ Ingestion Complete!")

if __name__ == "__main__":
    main()